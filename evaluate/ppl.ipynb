{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity Evaluation for Pythia-70M with KVPress\n",
    "\n",
    "This notebook evaluates the perplexity of the Pythia-70M model on Wikitext-2 and PG19 datasets, both with and without KV cache compression using KnormPress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:29:52.013084Z",
     "iopub.status.busy": "2025-12-05T07:29:52.013084Z",
     "iopub.status.idle": "2025-12-05T07:30:00.742415Z",
     "shell.execute_reply": "2025-12-05T07:30:00.741912Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "# Add the parent directory to sys.path to allow importing from accelerated_inference\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "\n",
    "from accelerated_inference.kvpress.presses.knorm_press import KnormPress\n",
    "from accelerated_inference.attention.patch import patch_attention_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:30:00.745419Z",
     "iopub.status.busy": "2025-12-05T07:30:00.745419Z",
     "iopub.status.idle": "2025-12-05T07:30:09.208263Z",
     "shell.execute_reply": "2025-12-05T07:30:09.207721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: EleutherAI/pythia-70m on cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"EleutherAI/pythia-70m\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATASET_DIR = \"../dataset\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME} on {DEVICE}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Apply attention patch\n",
    "patch_attention_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:30:09.211568Z",
     "iopub.status.busy": "2025-12-05T07:30:09.211027Z",
     "iopub.status.idle": "2025-12-05T07:30:09.216676Z",
     "shell.execute_reply": "2025-12-05T07:30:09.216113Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_ppl(model, tokenizer, text, seq_len=2048, stride=512, device=\"cuda\"):\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    max_length = model.config.max_position_embeddings\n",
    "    seq_len = min(seq_len, max_length)\n",
    "    \n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    \n",
    "    for begin_loc in tqdm(range(0, encodings.input_ids.size(1), stride), desc=\"Calculating PPL\"):\n",
    "        end_loc = min(begin_loc + seq_len, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - prev_end_loc\n",
    "        \n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            nlls.append(outputs.loss)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == encodings.input_ids.size(1):\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Evaluation (No Compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:30:09.219841Z",
     "iopub.status.busy": "2025-12-05T07:30:09.219841Z",
     "iopub.status.idle": "2025-12-05T07:34:36.825143Z",
     "shell.execute_reply": "2025-12-05T07:34:36.825143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikitext-2 from local disk...\n",
      "Total text length: 1294336 chars\n",
      "Evaluating Baseline on Wikitext-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  99%|█████████▉| 560/564 [00:23<00:00, 23.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Wikitext PPL: 39.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wikitext_path = os.path.join(DATASET_DIR, \"wikitext-2-raw-v1\")\n",
    "if os.path.exists(wikitext_path):\n",
    "    print(\"Loading Wikitext-2 from local disk...\")\n",
    "    data = load_from_disk(wikitext_path)\n",
    "    text = \"\\n\\n\".join(data[\"text\"])\n",
    "    print(f\"Total text length: {len(text)} chars\")\n",
    "    print(\"Evaluating Baseline on Wikitext-2...\")\n",
    "    ppl = calculate_ppl(model, tokenizer, text, device=DEVICE)\n",
    "    print(f\"Baseline Wikitext PPL: {ppl:.2f}\")\n",
    "else:\n",
    "    print(f\"Wikitext dataset not found at {wikitext_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with KnormPress (Compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:34:36.828157Z",
     "iopub.status.busy": "2025-12-05T07:34:36.828157Z",
     "iopub.status.idle": "2025-12-05T07:39:19.144176Z",
     "shell.execute_reply": "2025-12-05T07:39:19.144176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with KnormPress (Compression Ratio: 0.2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  99%|█████████▉| 560/564 [00:23<00:00, 24.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnormPress Wikitext PPL: 39.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compression_ratio = 0.2\n",
    "print(f\"Evaluating with KnormPress (Compression Ratio: {compression_ratio})...\")\n",
    "\n",
    "press = KnormPress(compression_ratio=compression_ratio)\n",
    "\n",
    "if os.path.exists(wikitext_path):\n",
    "    with press(model):\n",
    "        ppl = calculate_ppl(model, tokenizer, text, device=DEVICE)\n",
    "    print(f\"KnormPress Wikitext PPL: {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PG19 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:39:19.146269Z",
     "iopub.status.busy": "2025-12-05T07:39:19.146269Z",
     "iopub.status.idle": "2025-12-05T07:47:27.235916Z",
     "shell.execute_reply": "2025-12-05T07:47:27.235358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PG19 from local disk...\n",
      "Evaluating on first 5 samples of PG19...\n",
      "--- Baseline ---\n",
      "Processing sample 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  97%|█████████▋| 112/116 [00:04<00:00, 23.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 PPL: 29.36\n",
      "Processing sample 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  98%|█████████▊| 166/170 [00:06<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2 PPL: 50.69\n",
      "Processing sample 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  97%|█████████▋| 151/155 [00:06<00:00, 23.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 3 PPL: 40.74\n",
      "Processing sample 4/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  97%|█████████▋| 152/156 [00:06<00:00, 24.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 4 PPL: 57.71\n",
      "Processing sample 5/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  71%|███████▏  | 10/14 [00:00<00:00, 24.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 5 PPL: 29.40\n",
      "Average PG19 Baseline PPL: 41.58\n",
      "--- KnormPress (Ratio: 0.2) ---\n",
      "Processing sample 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  97%|█████████▋| 112/116 [00:04<00:00, 23.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 PPL: 29.36\n",
      "Processing sample 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  98%|█████████▊| 166/170 [00:07<00:00, 23.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2 PPL: 50.69\n",
      "Processing sample 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  97%|█████████▋| 151/155 [00:06<00:00, 23.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 3 PPL: 40.74\n",
      "Processing sample 4/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  97%|█████████▋| 152/156 [00:06<00:00, 23.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 4 PPL: 57.71\n",
      "Processing sample 5/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:  71%|███████▏  | 10/14 [00:00<00:00, 23.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 5 PPL: 29.40\n",
      "Average PG19 KnormPress PPL: 41.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pg19_path = os.path.join(DATASET_DIR, \"pg19\")\n",
    "if os.path.exists(pg19_path):\n",
    "    print(\"Loading PG19 from local disk...\")\n",
    "    txt_files = glob.glob(os.path.join(pg19_path, \"*.txt\"))\n",
    "    if len(txt_files) > 0:\n",
    "        data = []\n",
    "        for f in txt_files:\n",
    "            try:\n",
    "                with open(f, 'r', encoding='utf-8') as file:\n",
    "                    data.append({\"text\": file.read()})\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {f}: {e}\")\n",
    "\n",
    "        if len(data) > 0:\n",
    "            # Evaluate on a subset of PG19 to save time, e.g., first 5 books\n",
    "            num_samples = 5\n",
    "            print(f\"Evaluating on first {num_samples} samples of PG19...\")\n",
    "            \n",
    "            # Baseline\n",
    "            total_ppl_baseline = 0\n",
    "            count = 0\n",
    "            print(\"--- Baseline ---\")\n",
    "            for i, sample in enumerate(data):\n",
    "                if i >= num_samples:\n",
    "                    break\n",
    "                print(f\"Processing sample {i+1}/{num_samples}...\")\n",
    "                try:\n",
    "                    ppl = calculate_ppl(model, tokenizer, sample[\"text\"], device=DEVICE)\n",
    "                    print(f\"Sample {i+1} PPL: {ppl:.2f}\")\n",
    "                    total_ppl_baseline += ppl\n",
    "                    count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing sample {i+1}: {e}\")\n",
    "            \n",
    "            if count > 0:\n",
    "                print(f\"Average PG19 Baseline PPL: {total_ppl_baseline/count:.2f}\")\n",
    "            \n",
    "            # Compressed\n",
    "            total_ppl_compressed = 0\n",
    "            count = 0\n",
    "            print(f\"--- KnormPress (Ratio: {compression_ratio}) ---\")\n",
    "            with press(model):\n",
    "                for i, sample in enumerate(data):\n",
    "                    if i >= num_samples:\n",
    "                        break\n",
    "                    print(f\"Processing sample {i+1}/{num_samples}...\")\n",
    "                    try:\n",
    "                        ppl = calculate_ppl(model, tokenizer, sample[\"text\"], device=DEVICE)\n",
    "                        print(f\"Sample {i+1} PPL: {ppl:.2f}\")\n",
    "                        total_ppl_compressed += ppl\n",
    "                        count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing sample {i+1}: {e}\")\n",
    "            \n",
    "            if count > 0:\n",
    "                print(f\"Average PG19 KnormPress PPL: {total_ppl_compressed/count:.2f}\")\n",
    "\n",
    "        else:\n",
    "             print(\"No valid text files read from PG19.\")\n",
    "    else:\n",
    "        print(\"PG19 dataset is empty (no .txt files found).\")\n",
    "else:\n",
    "    print(f\"PG19 dataset not found at {pg19_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
